# DLS homeworks second half
___
В репозитории представлены ***мои*** решения домашних заданий со второй части курса [Deep Learning School](https://dls.samcs.ru/courses). 
Решения представлены в виде "как есть" и не являются оптимальными/лучшими и т.д. Редактироваться или исправляться, скорее всего, не будут.
Если вы проходите курс и хотите посмотреть возможные решения домашних заданий, то перед тем как смотреть их, убедитесь, что приложили все возможные усилия, чтобы выполнить их самостоятельно. Спасибо

## HW 4 Attention & finetune
___
Домашнее задание по теме "Механизм внимания"
* Написан кастомный класс для модели трансформера для задачи классификации. В качестве backbone использованы cointegrated/rubert-tiny2 и tbs17/MathBert с huggingface
* Написана функция для заморозки слоёв и дообучения трансформеров
* Дообучены модели трансформеров в варианте с заморозкой слоёв и без
* Отрисованы карты внимания голов внимания (attention heads) как для исходных моделей, так и для зафайнтюненных. Проведён анализ графиков.
___
## HW 3 RNN. Language modeling
Обучение языковой модели с помощью LSTM
* Собран word dataset из датасета отзывов с imdb при помощи библиотеки NLTK
* Обучены на генерацию текста 3 версии LSTM, 1 версия GRU и 1 версия RNN
* Полученные модели сравнены между собой по полученным графикам обучения и перплексии.
* Предложены способы улучшения качества генерации